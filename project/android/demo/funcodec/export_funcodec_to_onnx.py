#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆ FunCodec ç¼–ç å™¨å¯¼å‡ºä¸º ONNX - å½»åº•è§£å†³å½¢çŠ¶å›ºåŒ–é—®é¢˜
é€šè¿‡é‡æ–°è®¾è®¡æ•´ä¸ªå¯¼å‡ºæµç¨‹ï¼Œé¿å…æ‰€æœ‰å¯èƒ½çš„å›ºåŒ–æº

ç”¨æ³•:
    python export_funcodec_to_onnx.py --model_dir exp/audio_codec-encodec-en-libritts-16k-nq32ds640-pytorch --onnx_path funcodec_encoder.onnx
"""
import types, sys
fake_dynamo = types.ModuleType("torch._dynamo")
setattr(fake_dynamo, 'allow_in_graph', lambda *a, **kw: None)
setattr(fake_dynamo, 'trace_rules', {})
sys.modules["torch._dynamo"] = fake_dynamo

import argparse, os, yaml, torch
import torch.nn as nn
try:
    import soundfile as sf
except ImportError:
    sf = None
try:
    import librosa
except ImportError:
    librosa = None
from funcodec.tasks.gan_speech_codec import GANSpeechCodecTask
from argparse import Namespace

# ======================= ONNX EXPORT MONKEY PATCH (Definitive) =======================
# ç›®çš„: åœ¨ä¸ä¿®æ”¹FunCodecæºç çš„æƒ…å†µä¸‹ï¼Œä¿®å¤ONNXå¯¼å‡ºæ—¶çš„å½¢çŠ¶å›ºåŒ–é—®é¢˜ã€‚
# æ–¹æ³•: åŠ¨æ€æ›¿æ¢(çŒ´å­è¡¥ä¸)ç¼–ç å™¨æ ¸å¿ƒæ¨¡å— SConv1d çš„ forward æ–¹æ³•ã€‚
# é—®é¢˜æ ¹æº: SConv1d.forward åœ¨å¤„ç†éå› æœpaddingæ—¶ï¼Œå…¶ä¾èµ–çš„ F.pad
#           ä¸æ”¯æŒåŠ¨æ€paddingå°ºå¯¸ï¼Œä¸”å†…éƒ¨é€»è¾‘ä½¿ç”¨äº†math.ceilå’Œ
#           PythonåŸç”Ÿifåˆ¤æ–­ï¼Œå¯¼è‡´ONNXè¿½è¸ªæ—¶å°ºå¯¸è¢«å›ºåŒ–ã€‚
# æœ€ç»ˆè§£å†³æ–¹æ¡ˆ: é‡å†™ forward æ–¹æ³•ï¼Œç”¨ torch.cat æ‰‹åŠ¨å®ç°åŠ¨æ€paddingï¼Œ
#             å®Œå…¨ç»•å¼€ F.pad çš„é™åˆ¶ã€‚
print(">>> Applying DEFINITIVE ONNX export monkey patch for FunCodec SConv1d...")
import funcodec.modules.normed_modules.conv as conv_module
from torch.nn import functional as F

def _onnx_safe_sconv1d_forward(self, x):
    # self æ˜¯ SConv1d çš„å®ä¾‹
    kernel_size, = self.conv.conv.kernel_size
    stride, = self.conv.conv.stride
    dilation, = self.conv.conv.dilation
    
    if self.causal:
        # å› æœpaddingæ˜¯é™æ€çš„ï¼Œä½¿ç”¨åŸå§‹çš„pad1dæ˜¯å®‰å…¨çš„
        padding_total = (kernel_size - 1) * dilation
        x = conv_module.pad1d(x, (padding_total, 0), mode=self.pad_mode)
        return self.conv(x)
    else:
        # éå› æœpaddingï¼Œè¿™æ˜¯é—®é¢˜çš„æ ¸å¿ƒï¼Œéœ€è¦å®Œå…¨çš„åŠ¨æ€å®ç°
        padding_total_effective = (kernel_size - 1) * dilation - (stride - 1)
        
        # 1. åŠ¨æ€è®¡ç®— extra_padding (çº¯torchæ“ä½œ)
        length_tensor = x.new_tensor(x.shape[-1], dtype=torch.float32)
        n_frames = (length_tensor - kernel_size + padding_total_effective) / stride + 1
        ideal_length = (torch.ceil(n_frames) - 1) * stride + (kernel_size - padding_total_effective)
        extra_padding = (ideal_length - length_tensor).to(torch.long)

        # 2. è®¡ç®—å·¦å³padding
        padding_left = padding_total_effective // 2
        padding_right = padding_total_effective - padding_left
        
        # 3. æ‰‹åŠ¨å®ç°åŠ¨æ€paddingï¼Œç»•å¼€ F.pad
        # a. å·¦è¾¹padding (é™æ€)
        padded_x = F.pad(x, (padding_left, 0), mode=self.pad_mode)
        
        # b. å³è¾¹padding (åŠ¨æ€)
        # æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª (B, C, padding_right + extra_padding) çš„0å¼ é‡å¹¶æ‹¼æ¥
        # ä¸ºäº†åˆ›å»ºè¿™ä¸ªå¼ é‡ï¼Œæˆ‘ä»¬éœ€è¦å…¶åŠ¨æ€çš„shape
        right_pad_size = padding_right + extra_padding
        
        # åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿå¤§çš„é™æ€0å¼ é‡ï¼Œç„¶ååˆ‡ç‰‡ä»¥è·å¾—åŠ¨æ€å¤§å°çš„padding
        # è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„workaroundï¼Œä»¥é¿å…åœ¨ shape ä¸­ä½¿ç”¨åŠ¨æ€å¼ é‡
        # å‡è®¾æœ€å¤§paddingä¸ä¼šè¶…è¿‡ä¸€ä¸ªå¾ˆå¤§çš„æ•°ï¼Œä¾‹å¦‚2048
        # æ³¨æ„ï¼šè¿™å‡è®¾äº†åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œå³ä¾§çš„æ€»paddingä¸ä¼šè¶…è¿‡2048ã€‚
        # å¯¹äºéŸ³é¢‘æ¨¡å‹æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å®‰å…¨çš„å‡è®¾ã€‚
        max_padding_buffer = 2048 
        zeros_buffer = torch.zeros(x.shape[0], x.shape[1], max_padding_buffer, device=x.device, dtype=x.dtype)
        
        # ä»bufferä¸­åˆ‡å‡ºæˆ‘ä»¬éœ€è¦çš„åŠ¨æ€é•¿åº¦
        dynamic_padding_tensor = zeros_buffer.narrow(-1, 0, right_pad_size)
        
        # ä½¿ç”¨ torch.cat å®ç°æœ€ç»ˆçš„padding
        padded_x = torch.cat([padded_x, dynamic_padding_tensor], dim=-1)

        return self.conv(padded_x)

# 4. æ‰§è¡Œæ›¿æ¢
conv_module.SConv1d.forward = _onnx_safe_sconv1d_forward
print(">>> Monkey patch for SConv1d applied successfully.")
# ======================================================================

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument('--model_dir', required=True,
                   help='FunCodec é¢„è®­ç»ƒæ¨¡å‹ç›®å½•ï¼ˆé‡Œé¢åº”æœ‰ config.yaml å’Œ *.ptï¼‰')
    p.add_argument('--onnx_path', default='funcodec_encoder_ultimate.onnx')
    p.add_argument('--opset', type=int, default=14)
    p.add_argument('--dummy_wav', default='example2.wav',
                   help='ç”¨äºONNXå¯¼å‡ºçš„ç¤ºä¾‹wavæ–‡ä»¶ï¼›è‹¥ä¸å­˜åœ¨åˆ™ä½¿ç”¨éšæœºå™ªå£°')
    return p.parse_args()

def topdict_to_ns(d):
    return Namespace(**d)

def build_codec(model_dir):
    with open(os.path.join(model_dir, 'config.yaml'), 'r') as f:
        cfg_dict = yaml.safe_load(f)

    cfg = topdict_to_ns(cfg_dict)
    codec = GANSpeechCodecTask.build_model(cfg)
    
    weight_path = None
    for name in ["valid.loss.ave.pt", "train.loss.ave.pt"]:
        p = os.path.join(model_dir, name)
        if os.path.exists(p):
            weight_path = p
            break
    if weight_path is None:
        weight_path = sorted([p for p in os.listdir(model_dir) if p.endswith(".pth")])[-1]
        weight_path = os.path.join(model_dir, weight_path)

    state_dict = torch.load(weight_path, map_location="cpu")
    codec.load_state_dict(state_dict, strict=True)
    return codec

class UltimateEncoderWrapper(torch.nn.Module):
    """
    ç»ˆæç‰ˆç¼–ç å™¨åŒ…è£…å™¨ - å½»åº•è§£å†³å½¢çŠ¶å›ºåŒ–é—®é¢˜
    
    æ ¸å¿ƒç­–ç•¥ï¼š
    1. å®Œå…¨é¿å…torch.stack()å’Œtorch.cat()ç­‰å¯èƒ½å›ºåŒ–çš„æ“ä½œ
    2. ä½¿ç”¨æœ€åŸå§‹çš„å¼ é‡æ“ä½œ
    3. å¼ºåˆ¶ä½¿ç”¨åŠ¨æ€ç»´åº¦è®¡ç®—
    4. é¿å…ä»»ä½•å¯èƒ½è¢«ONNXä¼˜åŒ–å™¨å›ºåŒ–çš„æ“ä½œ
    """
    def __init__(self, codec):
        super().__init__()
        self.codec = codec
        self.encoder = codec.encoder
        self.quantizer = (
            codec.quantizer
            if hasattr(codec, "quantizer")
            else codec.rq
        )
        
        # è·å–é‡åŒ–å™¨å‚æ•°
        self.num_quantizers = 32  # å›ºå®šå€¼ï¼Œæ¥è‡ªé…ç½®
        self.hop_length = 640     # å›ºå®šå€¼ï¼Œæ¥è‡ªé…ç½®

    def forward(self, wav):
        # 1. è¾“å…¥é¢„å¤„ç†
        if wav.dim() == 2:
            wav = wav.unsqueeze(1)  # (B,1,T)

        # 2. éŸ³é¢‘å½’ä¸€åŒ–ï¼ˆæ¨¡æ‹ŸFunCodecçš„éŸ³é‡å½’ä¸€åŒ–ï¼‰
        mono = wav.mean(dim=1, keepdim=True)
        scale = torch.sqrt(mono.pow(2).mean(dim=2, keepdim=True) + 1e-8)
        wav_norm = wav / scale

        # 3. ç¼–ç 
        latent = self.encoder(wav_norm)  # (B,C,F)

        # 4. é‡åŒ– - è¿™æ˜¯å…³é”®æ­¥éª¤ï¼Œç¡®ä¿ä½¿ç”¨æ‰€æœ‰32å±‚
        # é‡åŒ–å™¨æœŸæœ›çš„è¾“å…¥æ ¼å¼æ˜¯ (B, C, T)ï¼Œä¿æŒç¼–ç å™¨è¾“å‡ºçš„åŸå§‹æ ¼å¼
        latent_for_quantizer = latent  # ä¿æŒ (B, C, T) æ ¼å¼
        
        # æ£€æŸ¥é‡åŒ–å™¨çš„forwardæ–¹æ³•ç­¾åå¹¶æ­£ç¡®è°ƒç”¨
        # ä¸åŒç‰ˆæœ¬çš„FunCodecé‡åŒ–å™¨å¯èƒ½æœ‰ä¸åŒçš„è°ƒç”¨æ–¹å¼
        try:
            # å°è¯•å¸¦å¸¦å®½å‚æ•°çš„è°ƒç”¨æ–¹å¼
            sample_rate = 16000
            bandwidth = None  # Noneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰32å±‚
            out = self.quantizer(latent_for_quantizer, sample_rate, bandwidth)
        except TypeError as e:
            print(f"å°è¯•ä¸‰å‚æ•°è°ƒç”¨å¤±è´¥: {e}")
            try:
                # å°è¯•ä¼ å…¥latentå’Œsample_rateçš„è°ƒç”¨æ–¹å¼
                sample_rate = 16000
                out = self.quantizer(latent_for_quantizer, sample_rate)
            except TypeError as e2:
                print(f"å°è¯•äºŒå‚æ•°è°ƒç”¨å¤±è´¥: {e2}")
                try:
                    # å°è¯•åªä¼ å…¥latentçš„è°ƒç”¨æ–¹å¼
                    out = self.quantizer(latent_for_quantizer)
                except TypeError as e3:
                    print(f"å°è¯•ä¸€å‚æ•°è°ƒç”¨å¤±è´¥: {e3}")
                    raise RuntimeError(f"æ— æ³•ç¡®å®šé‡åŒ–å™¨çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ã€‚é”™è¯¯: {e}, {e2}, {e3}")

        # 5. æå–codes - å½»åº•é‡å†™è¿™éƒ¨åˆ†é€»è¾‘
        codes = self._extract_codes_ultimate(out, wav.shape[-1])
        
        return codes.to(torch.int32)

    def _extract_codes_ultimate(self, quantizer_output, input_length):
        if hasattr(quantizer_output, 'codes'):
            codes = quantizer_output.codes  # (n_quantizers, B, T)
        elif isinstance(quantizer_output, (list, tuple)):
            if len(quantizer_output) >= 2:
                codes = quantizer_output[1]  # (quantized, codes, ...)
            elif len(quantizer_output) == 1:
                codes = quantizer_output[0]
            else:
                raise RuntimeError("é‡åŒ–å™¨è¾“å‡ºä¸ºç©ºçš„tuple/list")
        else:
            raise RuntimeError(f"æœªçŸ¥çš„é‡åŒ–å™¨è¾“å‡ºç±»å‹: {type(quantizer_output)}")

        # æ£€æŸ¥codesæ ¼å¼å¹¶è½¬æ¢ä¸ºAndroidæœŸæœ›çš„æ ¼å¼
        if isinstance(codes, (list, tuple)):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå †å ä¸º (n_quantizers, B, T)
            codes = torch.stack(codes, dim=0)
        
        # ç¡®ä¿codesæ˜¯ (n_quantizers, B, T) æ ¼å¼
        if codes.dim() == 2:
            # å¦‚æœæ˜¯2Dï¼Œéœ€è¦æ·»åŠ æ‰¹æ¬¡ç»´åº¦
            codes = codes.unsqueeze(0)  # (n_quantizers, T) -> (1, n_quantizers, T)
        
        return codes

def test_ultimate_dynamic_shapes(wrapper, device, test_lengths=[1.0, 2.0, 5.0, 10.0], sr=16000):
    """
    æµ‹è¯•ç»ˆæç‰ˆæœ¬çš„åŠ¨æ€å½¢çŠ¶æ”¯æŒ
    """
    print("\n" + "="*60)
    print("æµ‹è¯•ç»ˆæç‰ˆåŠ¨æ€å½¢çŠ¶æ”¯æŒ")
    print("="*60)
    
    results = []
    for length in test_lengths:
        samples = int(length * sr)
        test_wav = torch.randn(1, samples, dtype=torch.float32, device=device)
        
        with torch.no_grad():
            output = wrapper(test_wav)
        
        expected_frames = samples // 640  # hop_length = 640
        actual_frames = output.shape[2]
        
        print(f"é•¿åº¦: {length:4.1f}s ({samples:6d}æ ·æœ¬) -> è¾“å‡º: {list(output.shape)} "
              f"(æœŸæœ›å¸§æ•°: {expected_frames}, å®é™…å¸§æ•°: {actual_frames})")
        
        results.append((length, samples, output.shape, expected_frames, actual_frames))
    
    # éªŒè¯åŠ¨æ€æ€§
    frame_ratios = [r[4] / r[3] for r in results]  # actual / expected
    if len(set([r[2] for r in results])) == 1:
        print("âŒ å½¢çŠ¶å®Œå…¨ç›¸åŒï¼Œå¯èƒ½ä»ç„¶å›ºåŒ–")
        return False
    else:
        print("âœ… å½¢çŠ¶éšè¾“å…¥å˜åŒ–ï¼ŒåŠ¨æ€æ€§æ­£å¸¸")
        return True

def main():
    args = parse_args()
    device = 'cpu'
    
    print("="*60)
    print("ç»ˆæç‰ˆ FunCodec ONNX å¯¼å‡º")
    print("="*60)
    
    # æ„å»ºæ¨¡å‹
    codec = build_codec(args.model_dir).to(device).eval()
    wrapper = UltimateEncoderWrapper(codec).to(device).eval()

    # æ„é€  dummy è¾“å…¥
    sr = 16000
    if sf is not None and os.path.isfile(args.dummy_wav):
        wav, sr_file = sf.read(args.dummy_wav)
        if sr_file != sr:
            if librosa is None:
                raise RuntimeError("éœ€è¦ librosa è¿›è¡Œé‡é‡‡æ ·")
            wav = librosa.resample(y=wav, orig_sr=sr_file, target_sr=sr)
        wav = torch.from_numpy(wav).float().unsqueeze(0)
        print(f"ä½¿ç”¨éŸ³é¢‘æ–‡ä»¶: {args.dummy_wav}, é•¿åº¦: {wav.shape[1]} samples")
    else:
        wav = torch.randn(1, sr, dtype=torch.float32)
        print("ä½¿ç”¨éšæœºå™ªå£° 1s")
    
    dummy = wav.to(device)

    # æµ‹è¯•PyTorchæ¨¡å‹çš„åŠ¨æ€å½¢çŠ¶æ”¯æŒ
    print("\næµ‹è¯•PyTorchæ¨¡å‹:")
    pytorch_dynamic = test_ultimate_dynamic_shapes(wrapper, device)
    
    if not pytorch_dynamic:
        print("âŒ PyTorchæ¨¡å‹æœ¬èº«ä¸æ”¯æŒåŠ¨æ€å½¢çŠ¶ï¼Œæ— æ³•ç»§ç»­")
        return

    print(f'\nå¼€å§‹å¯¼å‡º ONNX åˆ° {args.onnx_path} ...')
    
    # å¯¼å‡ºONNX - ä½¿ç”¨æœ€ä¿å®ˆçš„è®¾ç½®
    torch.onnx.export(
        wrapper,
        dummy,
        args.onnx_path,
        opset_version=args.opset,
        input_names=['waveform'],
        output_names=['codes'],
        dynamic_axes={
            'waveform': {1: 'n_samples'},
            'codes': {2: 'n_frames'}
        },
        do_constant_folding=False,      # å…³é—­å¸¸é‡æŠ˜å 
        keep_initializers_as_inputs=False,
        verbose=False,
        training=torch.onnx.TrainingMode.EVAL
    )
    print(f'ONNX å¯¼å‡ºå®Œæˆ!')

    # éªŒè¯ONNXæ¨¡å‹çš„åŠ¨æ€å½¢çŠ¶
    print("\n" + "="*60)
    print("éªŒè¯ ONNX æ¨¡å‹åŠ¨æ€å½¢çŠ¶")
    print("="*60)
    
    import onnx, onnxruntime
    
    # æ£€æŸ¥æ¨¡å‹
    onnx_model = onnx.load(args.onnx_path)
    onnx.checker.check_model(onnx_model)
    print("âœ… ONNXæ¨¡å‹ç»“æ„æ£€æŸ¥é€šè¿‡")
    
    # åˆ›å»ºè¿è¡Œæ—¶ä¼šè¯
    ort_sess = onnxruntime.InferenceSession(
        args.onnx_path, 
        providers=['CPUExecutionProvider']
    )
    
    # æµ‹è¯•å¤šä¸ªé•¿åº¦
    test_lengths = [1.0, 2.0, 5.0, 10.0]
    onnx_shapes = []
    pytorch_shapes = []
    
    all_passed = True
    
    for length in test_lengths:
        samples = int(length * sr)
        test_wav = torch.randn(1, samples, dtype=torch.float32)
        
        # PyTorchæ¨ç†
        with torch.no_grad():
            pytorch_output = wrapper(test_wav.to(device)).cpu().numpy()
        
        # ONNXæ¨ç†
        try:
            onnx_output = ort_sess.run(None, {'waveform': test_wav.numpy()})[0]
        except Exception as e:
            print(f"âŒ ONNXæ¨ç†å¤±è´¥ ({length}s): {e}")
            all_passed = False
            continue
        
        pytorch_shapes.append(pytorch_output.shape)
        onnx_shapes.append(onnx_output.shape)
        
        print(f"é•¿åº¦: {length:4.1f}s -> PyTorch: {pytorch_output.shape}, ONNX: {onnx_output.shape}")
        
        if pytorch_output.shape != onnx_output.shape:
            print(f"  âŒ å½¢çŠ¶ä¸åŒ¹é…!")
            all_passed = False
        else:
            print(f"  âœ… å½¢çŠ¶åŒ¹é…")
    
    # æ£€æŸ¥ONNXè¾“å‡ºæ˜¯å¦çœŸçš„åŠ¨æ€
    if len(set(onnx_shapes)) == 1:
        print(f"\nâŒ ONNXè¾“å‡ºå½¢çŠ¶å›ºåŒ–: æ‰€æœ‰æµ‹è¯•éƒ½äº§ç”Ÿç›¸åŒå½¢çŠ¶ {onnx_shapes[0]}")
        all_passed = False
    else:
        print(f"\nâœ… ONNXè¾“å‡ºå½¢çŠ¶åŠ¨æ€: äº§ç”Ÿäº† {len(set(onnx_shapes))} ç§ä¸åŒå½¢çŠ¶")
    
    if all_passed:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! ç»ˆæç‰ˆONNXæ¨¡å‹æ”¯æŒçœŸæ­£çš„åŠ¨æ€å½¢çŠ¶!")
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼Œæ¨¡å‹å¯èƒ½ä»å­˜åœ¨å›ºåŒ–é—®é¢˜")
    
    return all_passed

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1) 